{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da3937-c0a7-48e4-83be-0ff1e90311bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import gym\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "from baselines.common.atari_wrappers import wrap_deepmind\n",
    "\n",
    "discount_factor = 0.99 #gamma\n",
    "learning_rate = 0.0002\n",
    "\n",
    "#Epsilon greedy params\n",
    "epsilon = 0.89\n",
    "epsilon_min = 0.1 \n",
    "epsilon_decay = ( 1 - epsilon_min) / (1000000) #Epsilon will drecrease by this amount at every step\n",
    "\n",
    "#Exp replay\n",
    "max_exp_replay_size = 100000\n",
    "batch_dim = 8 \n",
    "\n",
    "#Steps between each update\n",
    "steps_update_target_net = 5000\n",
    "\n",
    "max_steps_episode = 25000\n",
    "max_steps = 1000000 #1 million maximum total training steps\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "loss_function = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c6be9f-522c-4890-8d99-59c36227f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "\n",
    "#Using no frameskip version, as it will be then made into frameskip (n=4) by the make_atari function\n",
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\") \n",
    "\n",
    "#Reduce frame size and stacks 4 of them\n",
    "env = wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=True, scale=True)\n",
    "    #Clip_rewards: rewards are transformed in the [-1,1] format\n",
    "    #Episodic_life: considers losing a life as the end of an episode. Used by deepmind as it helps value estimation\n",
    "\n",
    "env.seed(42)\n",
    "num_possible_actions = env.action_space.n\n",
    "\n",
    "print(\"Environment input shape: {}\".format(env.observation_space.shape))\n",
    "print(\"Environment output shape: {}\".format(env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b49ad-4918-4239-af90-07093d2fd5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(84, 84, 4,))\n",
    "\n",
    "    l1 = layers.Conv2D(filters = 32, kernel_size = 8, strides=4, activation=\"relu\")(inputs)\n",
    "    l2 = layers.Conv2D(filters = 64, kernel_size = 4, strides=2, activation=\"relu\")(l1)\n",
    "    l3 = layers.Conv2D(filters = 64, kernel_size = 3, strides=1, activation=\"relu\")(l2)\n",
    "\n",
    "    l4 = layers.Flatten()(l3)\n",
    "\n",
    "    l5 = layers.Dense(units = 512, activation=\"relu\")(l4)\n",
    "    \n",
    "    v_stream = layers.Dense(units = 512,activation='relu')(l5)\n",
    "    v_stream = layers.Dense(units = 1, activation='linear',name=\"Value\")(v_stream)\n",
    "    v_stream = layers.RepeatVector(num_possible_actions)(v_stream)\n",
    "    v_stream = layers.Flatten()(v_stream)\n",
    "\n",
    "    a_stream = layers.Dense(units = 512,activation='relu')(l5)\n",
    "    a_stream = layers.Dense(num_possible_actions, activation='linear',name='Activation')(a_stream)\n",
    "\n",
    "    mean_a_stream = layers.RepeatVector(num_possible_actions)(tensorflow.keras.backend.mean(a_stream,axis=1,keepdims=True))\n",
    "    mean_a_stream = layers.Flatten(name='meanActivation')(mean_a_stream)\n",
    "\n",
    "    a_stream = layers.Subtract()([a_stream,mean_a_stream])\n",
    "\n",
    "    head_q = layers.Add(name = \"Q-value\")([v_stream, a_stream])\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=head_q)\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "target_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef381a6-c83f-455c-8a32-a3089f2e29a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only if one wants to resume training from save weights\n",
    "model.load_weights(\"\")\n",
    "target_model.load_weights(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0da551-5a17-431a-a90e-7e818ce9d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9d5ed-7e40-41b8-bee0-42594073a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(epsilon, state):\n",
    "    #Epsilon greedy \n",
    "    if epsilon > random.random():\n",
    "        action = random.randrange(0, num_possible_actions)\n",
    "    else:\n",
    "        #Predict Q-values of actions and take best one\n",
    "        tensorized_state = tensorflow.convert_to_tensor(state)\n",
    "        tensorized_state = tensorflow.expand_dims(tensorized_state, 0)\n",
    "        action_probabilities = model(tensorized_state, training=False)\n",
    "        action = tensorflow.argmax(action_probabilities[0]).numpy()\n",
    "    return action\n",
    "            \n",
    "            \n",
    "def memorize(action,state,new_state,done,reward):\n",
    "    action_buffer.append(action)\n",
    "    state_buffer.append(state)\n",
    "    new_state_buffer.append(new_state)\n",
    "    done_buffer.append(done)\n",
    "    reward_buffer.append(reward)\n",
    "\n",
    "def train():\n",
    "    #Randomly extract values from buffers\n",
    "    #index = numpy.random.choice(range(len(done_buffer)), size=batch_dim)\n",
    "    index = random.sample(range(len(done_buffer)), batch_dim)\n",
    "\n",
    "    state_batch = numpy.array([state_buffer[i] for i in index])\n",
    "    new_state_batch = numpy.array([new_state_buffer[i] for i in index])\n",
    "    reward_batch = [reward_buffer[i] for i in index]\n",
    "    action_batch = [action_buffer[i] for i in index]\n",
    "    done_batch = tensorflow.convert_to_tensor([float(done_buffer[i]) for i in index])\n",
    "\n",
    "    #Calculate new q values\n",
    "    next_rewards_batch = target_model.predict(new_state_batch)\n",
    "    new_q_vals = reward_batch + discount_factor * numpy.amax(next_rewards_batch)\n",
    "\n",
    "    #If it was the final step change value to -1\n",
    "    new_q_vals = new_q_vals * (1 - done_batch) - done_batch\n",
    "\n",
    "    mask = tensorflow.one_hot(action_batch, num_possible_actions)\n",
    "\n",
    "    with tensorflow.GradientTape() as t:\n",
    "        q_vals = model(state_batch)\n",
    "        q_action = tensorflow.reduce_sum(tensorflow.multiply(q_vals, mask), axis=1)\n",
    "        loss = loss_function(new_q_vals, q_action)\n",
    "\n",
    "    gradients = t.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b492e80-aa7c-4327-a332-3c182dfb43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history = []\n",
    "running_reward_history = []\n",
    "episode_duration_history = []\n",
    "\n",
    "mean_reward_100ep = 0\n",
    "mean_duration_100ep = 0\n",
    "episode_counter = 0\n",
    "step_counter = 0\n",
    "current_max_reward = 0\n",
    "\n",
    "action_buffer = []\n",
    "state_buffer = []\n",
    "new_state_buffer = []\n",
    "reward_buffer = []\n",
    "done_buffer = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"starting...\")\n",
    "while True: \n",
    "    tot_ep_reward = 0\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    cur_state = numpy.asarray(env.reset())\n",
    "    \n",
    "    for timestep in range(1, max_steps_episode+1):\n",
    "    \n",
    "        #env.render(); \n",
    "        \n",
    "        step_counter += 1\n",
    "\n",
    "        #Choose next action\n",
    "        action = get_action(epsilon, cur_state)\n",
    "        \n",
    "        #Value of epsilon decreases\n",
    "        epsilon -= epsilon_decay\n",
    "        if epsilon < epsilon_min:\n",
    "            epsilon = epsilon_min\n",
    "\n",
    "        # do one step with selected action\n",
    "        new_state, reward, is_done, _ = env.step(action)\n",
    "        new_state = numpy.asarray(new_state)\n",
    "\n",
    "        tot_ep_reward += reward\n",
    "\n",
    "        \n",
    "        # memorize info in rep buffer\n",
    "        memorize(action,cur_state,new_state,is_done,reward)\n",
    "        \n",
    "        cur_state = new_state\n",
    "\n",
    "        # when batch is large enough train the model\n",
    "        if len(done_buffer) > batch_dim:\n",
    "            train()\n",
    "            \n",
    "            \n",
    "        #every n steps update target netork\n",
    "        if step_counter % steps_update_target_net == 0:\n",
    "            target_model.set_weights(model.get_weights())\n",
    "            print(\"{}/{} steps, {} episodes. avg reward for last 100 episodes: {:.2f}, Epsilon: {}, avg epoch duration: {:.2f}\".format(step_counter, max_steps, episode_counter, mean_reward_100ep, epsilon, mean_duration_100ep))\n",
    "                        \n",
    "        # keep buffer size below max\n",
    "        if len(reward_buffer) > max_exp_replay_size:\n",
    "            del reward_buffer[:1]\n",
    "            del state_buffer[:1]\n",
    "            del new_state_buffer[:1]\n",
    "            del action_buffer[:1]\n",
    "            del done_buffer[:1]\n",
    "            \n",
    "        if is_done == True:\n",
    "            break\n",
    "\n",
    "            \n",
    "    episode_duration_history.append(time.time() - epoch_start_time)\n",
    "    mean_duration_100ep = numpy.mean(episode_duration_history[-100:])\n",
    "\n",
    "    # add rewards to history\n",
    "    reward_history.append(tot_ep_reward)\n",
    "\n",
    "    #determine avg reward of past n ep.\n",
    "    mean_reward_100ep = numpy.mean(reward_history[-100:])\n",
    "\n",
    "    #insert running reward in running reward history\n",
    "    running_reward_history.append(mean_reward_100ep)\n",
    "\n",
    "    episode_counter += 1\n",
    "    \n",
    "    #save weights if it reached new record running average\n",
    "    if step_counter > 20000 and mean_reward_100ep > current_max_reward:\n",
    "        current_max_reward = mean_reward_100ep\n",
    "        s = \"0\"*(15-len(str(step_counter)))+str(step_counter)\n",
    "        r = \"0\"*(5-len(str(round(mean_reward_100ep, 2))))+str(round(mean_reward_100ep, 2))\n",
    "        model.save_weights(\"./weights/DDQN/r_{}_s_{}/model/weights\".format(r, s))\n",
    "        target_model.save_weights(\"./weights/DDQN/r_{}_s_{}/target/weights\".format(r,s))\n",
    "        print(\"reached new record of {:.2f}, saved weights as DDQN_r_{}_s_{}\".format(mean_reward_100ep,r,s))\n",
    "    \n",
    "    #end condition 3: reached step limit\n",
    "    if step_counter > max_steps: \n",
    "        print(\"max number of steps {} reached at episode {}, avg reward is: {}\".format(max_steps, episode_counter, objective_mean_reward_100ep))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17ab0c-19e2-436e-87df-3765381edd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(running_reward_history)\n",
    "plt.ylabel('running reward')\n",
    "plt.xlabel('steps divided by 5000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba48f9c0-1cfd-42a2-a9e8-c6d879d3c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(reward_history)\n",
    "plt.ylabel('reward')\n",
    "plt.xlabel('steps divided by 5000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6635b905-711d-4f00-88d7-030d3ccef497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trained net testing\n",
    "\n",
    "test_model = create_model()\n",
    "\n",
    "test_model.load_weights(\"./weights/DDQN/r_10.85_s_000000001119429/target/weights\")\n",
    "#Baseline testing\n",
    "history = []\n",
    "\n",
    "print(\"starting...\")\n",
    "for episode in range(1, 100):\n",
    " \n",
    "    cur_state = np.asarray(env.reset())\n",
    "    tot_ep_reward = 0\n",
    "    \n",
    "    for timestep in range(1, 25000):\n",
    "    \n",
    "        #env.render(); \n",
    "        action = get_action(0, cur_state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.asarray(next_state)\n",
    "        \n",
    "        tot_ep_reward += reward\n",
    "\n",
    "        cur_state = next_state\n",
    "    \n",
    "        if done == True:\n",
    "            #print(tot_ep_reward)\n",
    "            history.append(tot_ep_reward)\n",
    "            break\n",
    "    \n",
    "    #calculate reward of last 100 episodes\n",
    "    mean_reward_100ep = np.mean(history)\n",
    "\n",
    "print(\"mean reward of 100 episodes is: {}\".format(mean_reward_100ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1779f93-79bb-4f8d-88f3-a0c5bfe63222",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.amax(history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
